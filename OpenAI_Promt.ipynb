{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE A CHATBOT WITH OPENAI API\n",
    "\n",
    "First, we need to set up to use the OpenAI API\n",
    "- Log in to OpenAI platform\n",
    "- Billing information\n",
    "- Set up the usage limit is important\n",
    "- Create the secret key to call OpenAI, this key only appears to see once, so copy and save for the future use\n",
    "- Get the OpenAI key and save in the file mykey.py in the same folder\n",
    "*********\n",
    "Install packages:\n",
    "+ %pip install llama\n",
    "+ %pip install docx2txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Devices:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "GPU details:  {'device_name': 'METAL'}\n"
     ]
    }
   ],
   "source": [
    "#This code to enable GPU for your laptop if you have, if not you can run the bellow cell.\n",
    "\n",
    "import tensorflow as tf\n",
    "devices = tf.config.list_physical_devices()\n",
    "print(\"\\nDevices: \", devices)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    details = tf.config.experimental.get_device_details(gpus[0])\n",
    "    print(\"GPU details: \", details)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hnguyen1/Library/CloudStorage/OneDrive-MichiganStateUniversity/PD/LLM/ShawYoutube/OpenAI\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os \n",
    "import mykey\n",
    "\n",
    "#Declare the Open_API_Key for the envi\n",
    "os.environ[\"OPENAI_API_KEY\"] = mykey.openai_key\n",
    "\n",
    "cur_dir = os.getcwd() \n",
    "\n",
    "print(cur_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You only need to run one of the block below. They are the same but different way to write the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK A:\n",
    "# We need to read data (all files in the folder ./data/) this is using the SimpleDirectoryReader of LLama.core\n",
    "# After reading the files, the data needed to be indexed using the format of the GPT model\n",
    "# Save the index for future use\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader, GPTVectorStoreIndex\n",
    "\n",
    "reader = SimpleDirectoryReader(\n",
    "    input_dir=cur_dir + \"/data/\"\n",
    ")\n",
    "\n",
    "documents = reader.load_data()\n",
    "\n",
    "#After reading the file, we need to create an index for the file using GPTVectorStoreIndex. \n",
    "## This step to help the retrieving process\n",
    "\n",
    "myindex = GPTVectorStoreIndex.from_documents(documents)\n",
    "\n",
    "#This index can be saved in the disk for future use \n",
    "\n",
    "myindex.storage_context.persist('')\n",
    "\n",
    "# Save the index\n",
    "myindex.storage_context.persist(cur_dir+\"/index/\")\n",
    "\n",
    "#!!!!!!!!!!!!!!!!#\n",
    "\n",
    "#Next time when running the chatbot, you can just load the index without indexing the documents\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "storage = StorageContext.from_defaults(persist_dir=cur_dir+\"/index/\")\n",
    "\n",
    "myindex = load_index_from_storage(storage)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BLOCK B\n",
    "#This code basically do the same, different code is used in this block.\n",
    "#This code is from the document of Llama index \n",
    "\n",
    "import os.path\n",
    "from llama_index. core import VectorStoreIndex, SimpleDirectoryReader, StorageContext, load_index_from_storage\n",
    "\n",
    "#Now we check if the data index already exists or not:\n",
    "#----If not, then we have to read, index and save. Those file are jason files. \n",
    "#----If yes, then we only need to read from the storage. \n",
    "\n",
    "index_dir = cur_dir +\"/index/\"\n",
    "\n",
    "if not os.path.exists(index_dir):\n",
    "    #create the index\n",
    "    os.makedirs(index_dir)\n",
    "    #read the documents from the data folder and create indexes\n",
    "    docs = SimpleDirectoryReader(input_dir=cur_dir + \"/data/\").load_data()\n",
    "    #indexes the data using such as world2vec \n",
    "    index = VectorStoreIndex.from_documents(docs)\n",
    "    #store it for later use\n",
    "    index.storage_context.persist(persist_dir=index_dir)\n",
    "else:\n",
    "    #the documents are already saved\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=index_dir)\n",
    "    index = load_index_from_storage(storage_context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we are going to build our chatbot Lucy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "\n",
    "class MyChatbot:\n",
    "    def __init__(self, api_key, index):\n",
    "        openai.api_key = api_key\n",
    "        self.index = index\n",
    "        self.chat_history = []\n",
    "    \n",
    "    def gen_response(self, question):\n",
    "        query_engine = index.as_query_engine()\n",
    "        response = query_engine.query(question)\n",
    "        \n",
    "        prompt = \"\\n\".join([f\"{message['role']}:{message['content']}\" \n",
    "                            for message in self.chat_history[-10:]\n",
    "                          ])\n",
    "        \n",
    "        prompt += f\"\\n User: {question}\"\n",
    "        \n",
    "        query_eng = index.as_query_engine()\n",
    "        res = query_eng.query(question)\n",
    "        \n",
    "        message = {\"role\": \"helper\", \"content\": res.response}\n",
    "        \n",
    "        #save in chat history\n",
    "        self.chat_history.append({\"role\": \"user\", \"content\": question})\n",
    "        self.chat_history.append(message)\n",
    "        \n",
    "        return message\n",
    "    \n",
    "    def load_chat_history(self, fname):\n",
    "        try:\n",
    "            with open(fname, 'r') as f:\n",
    "                self.chat_history = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "    \n",
    "    def save_chat_history(self, fname):\n",
    "        with open(fname, 'w') as f:\n",
    "            json.dump(self.chat_history, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Lucy chatbot is working now "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: tell me what time is the class math1750-030\n",
      "Lucy: The class Math1750-030 time is not specified in the provided context information.\n",
      "You: hanh nguyen is the instructor of what class\n",
      "Lucy: Calculus 1\n",
      "You: tell me where is hanh nguyen office\n",
      "Lucy: I'm sorry, but based on the provided context information, there is no mention of Hanh Nguyen's office location.\n",
      "You: hanh nguyen is the instructor of calculus 1 math 1750, where is her office\n",
      "Lucy: The office location for Hanh Nguyen, the instructor of Calculus 1 (MATH 1750), is not provided in the given context information.\n",
      "You: bye\n",
      "Lucy: I'm here to help whenever you need assistance.\n",
      "Lucy: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "#The mykey.openai_key is saved in mykey.py file\n",
    "#index is the index created by llama.index with GPT structure \n",
    "\n",
    "mybot = MyChatbot(mykey.openai_key, index)\n",
    "\n",
    "bot = MyChatbot(mykey.openai_key, index=index)\n",
    "bot.load_chat_history(\"chat_history.json\")\n",
    "\n",
    "#while True:\n",
    "question =  \"\"\n",
    "\n",
    "while True:\n",
    "    if question.lower() in [\"bye\", \"goodbye\", \"done\", \"thanks\"]:\n",
    "        print(\"Lucy: Goodbye!\")\n",
    "        bot.save_chat_history(\"chat_history.json\")\n",
    "        break\n",
    "    question = input(\"You: \")\n",
    "    response = bot.gen_response(question)\n",
    "    print(f\"Lucy: {response['content']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
